{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amathkur/Ardupilot_Multiagent_Simulation/blob/main/Copy_of_Untitled2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 793
        },
        "id": "RbQMQFHJvHmK",
        "outputId": "f516f839-f1e6-4411-b3d8-9293f4f9be6e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for SAM-2 (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.9/49.9 MB\u001b[0m \u001b[31m50.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDevice: cpu\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://6becbdee213adc7667.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://6becbdee213adc7667.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# ============================================\n",
        "# SAM2 (CPU-safe) + Single-Object Tracking + MiDaS Point Clouds\n",
        "# ============================================\n",
        "# Env: CPU SAFE\n",
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"   # hard-disable CUDA to avoid torch cuda init on CPU runtimes\n",
        "\n",
        "# ---------------- Install deps ----------------\n",
        "!pip install -q \"torch>=2.5.1\" \"torchvision>=0.20.1\" --index-url https://download.pytorch.org/whl/cpu\n",
        "!pip install -q git+https://github.com/facebookresearch/sam2.git\n",
        "!pip install -q opencv-python-headless==4.10.0.84 numpy tqdm timm gradio\n",
        "\n",
        "import cv2, numpy as np, torch, gradio as gr, tempfile, json\n",
        "from tqdm import tqdm\n",
        "from sam2.sam2_video_predictor import SAM2VideoPredictor\n",
        "\n",
        "device = \"cpu\"  # enforced\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# ---------- Globals shared across steps ----------\n",
        "SESSION = {\n",
        "    \"video_path\": None,\n",
        "    \"first_frame_path\": None,\n",
        "    \"bbox_xywh\": None,\n",
        "    \"out_dir\": None,\n",
        "    \"masked_video_path\": None,\n",
        "    \"mask_dump_dir\": None,\n",
        "    \"W\": None, \"H\": None, \"N\": None, \"fps\": 30.0,\n",
        "}\n",
        "\n",
        "# ---------- Helper: write ASCII PLY ----------\n",
        "def write_ply_ascii_xyzrgb(path, xyz, rgb):\n",
        "    n = xyz.shape[0]\n",
        "    with open(path, \"w\") as f:\n",
        "        f.write(\"ply\\nformat ascii 1.0\\n\")\n",
        "        f.write(f\"element vertex {n}\\n\")\n",
        "        f.write(\"property float x\\nproperty float y\\nproperty float z\\n\")\n",
        "        f.write(\"property uchar red\\nproperty uchar green\\nproperty uchar blue\\n\")\n",
        "        f.write(\"end_header\\n\")\n",
        "        for (x,y,z),(r,g,b) in zip(xyz, rgb):\n",
        "            f.write(f\"{x:.6f} {y:.6f} {z:.6f} {int(r)} {int(g)} {int(b)}\\n\")\n",
        "\n",
        "# ---------- UI Step 1: Load video & show first frame ----------\n",
        "def load_video(file):\n",
        "    if file is None:\n",
        "        return gr.update(value=None), \"No video uploaded.\", None\n",
        "    vid_path = file.name\n",
        "    cap = cv2.VideoCapture(vid_path)\n",
        "    ok, frame = cap.read()\n",
        "    if not ok:\n",
        "        cap.release()\n",
        "        return gr.update(value=None), \"Cannot read first frame.\", None\n",
        "    H, W = frame.shape[:2]\n",
        "    N = int(cap.get(cv2.CAP_PROP_FRAME_COUNT) or 0)\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS) or 30.0\n",
        "    cap.release()\n",
        "\n",
        "    first_frame_path = os.path.join(tempfile.mkdtemp(), \"first_frame.jpg\")\n",
        "    cv2.imwrite(first_frame_path, frame[:, :, ::-1][:, :, ::-1])  # ensure BGR write\n",
        "\n",
        "    base = os.path.splitext(os.path.basename(vid_path))[0]\n",
        "    out_dir = os.path.join(\"/content\", base + \"_sam2_single_obj\")\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "    mask_dump_dir = os.path.join(out_dir, \"masks_png\")\n",
        "    os.makedirs(mask_dump_dir, exist_ok=True)\n",
        "    masked_video_path = os.path.join(out_dir, \"masked_object_only.mp4\")\n",
        "\n",
        "    SESSION.update({\n",
        "        \"video_path\": vid_path,\n",
        "        \"first_frame_path\": first_frame_path,\n",
        "        \"W\": W, \"H\": H, \"N\": N, \"fps\": fps,\n",
        "        \"out_dir\": out_dir,\n",
        "        \"mask_dump_dir\": mask_dump_dir,\n",
        "        \"masked_video_path\": masked_video_path\n",
        "    })\n",
        "    msg = f\"Loaded video: {vid_path}\\nResolution: {W}x{H}, Frames: {N}, FPS: {fps:.2f}\\nDraw a box, then click 'Run Part 1'.\"\n",
        "    return first_frame_path, msg, json.dumps(SESSION, indent=2)\n",
        "\n",
        "# ---------- UI Step 2: Receive bbox from Gradio (select tool returns (x,y,w,h)) ----------\n",
        "def set_bbox(bbox_json):\n",
        "    # bbox_json format from gradio Image(tool=\"select\"): {'x':..., 'y':..., 'width':..., 'height':...}\n",
        "    if bbox_json is None:\n",
        "        return \"No box selected.\"\n",
        "    x = int(bbox_json.get(\"x\", 0))\n",
        "    y = int(bbox_json.get(\"y\", 0))\n",
        "    w = int(bbox_json.get(\"width\", 0))\n",
        "    h = int(bbox_json.get(\"height\", 0))\n",
        "    if w <= 0 or h <= 0:\n",
        "        return \"Invalid box.\"\n",
        "    SESSION[\"bbox_xywh\"] = (x, y, w, h)\n",
        "    return f\"BBox set to (x={x}, y={y}, w={w}, h={h}).\"\n",
        "\n",
        "# ---------- Part 1: SAM2 tracking on CPU ----------\n",
        "def run_part1():\n",
        "    if SESSION[\"video_path\"] is None:\n",
        "        return \"Upload a video first.\"\n",
        "    if SESSION[\"bbox_xywh\"] is None:\n",
        "        return \"Select a bounding box first.\"\n",
        "\n",
        "    video_path = SESSION[\"video_path\"]\n",
        "    out_dir = SESSION[\"out_dir\"]\n",
        "    mask_dump_dir = SESSION[\"mask_dump_dir\"]\n",
        "    masked_video_path = SESSION[\"masked_video_path\"]\n",
        "    W, H, N, fps = SESSION[\"W\"], SESSION[\"H\"], SESSION[\"N\"], SESSION[\"fps\"]\n",
        "    x, y, w, h = SESSION[\"bbox_xywh\"]\n",
        "\n",
        "    # Load smaller SAM2 checkpoint to be CPU-friendlier\n",
        "    predictor = SAM2VideoPredictor.from_pretrained(\"facebook/sam2-hiera-small\")\n",
        "    predictor = predictor.to(device)\n",
        "\n",
        "    # Init predictor state\n",
        "    state = predictor.init_state(video_path)\n",
        "\n",
        "    # Add the bbox as prompt\n",
        "    box_prompt = np.array([[x, y, x+w, y+h]], dtype=np.float32)\n",
        "    predictor.add_new_points_or_box(state, {\"box\": box_prompt})\n",
        "\n",
        "    # Prepare writer\n",
        "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
        "    vw = cv2.VideoWriter(masked_video_path, fourcc, fps, (W, H))\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "    pbar = tqdm(total=N, desc=\"SAM2 Tracking (CPU)\")\n",
        "    for f_idx, obj_ids, mask_list in predictor.propagate_in_video(state):\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, f_idx)\n",
        "        ok, frame_bgr = cap.read()\n",
        "        if not ok or not mask_list:\n",
        "            pbar.update(1); continue\n",
        "\n",
        "        mask = np.asarray(mask_list[0], dtype=bool)\n",
        "        masked = np.zeros_like(frame_bgr)\n",
        "        masked[mask] = frame_bgr[mask]\n",
        "        vw.write(masked)\n",
        "\n",
        "        mask_png = (mask.astype(np.uint8) * 255)\n",
        "        cv2.imwrite(os.path.join(mask_dump_dir, f\"mask_{f_idx:05d}.png\"), mask_png)\n",
        "        pbar.update(1)\n",
        "    pbar.close()\n",
        "    cap.release()\n",
        "    vw.release()\n",
        "\n",
        "    return f\"✅ Part 1 done.\\nMasked video: {masked_video_path}\\nMasks dir: {mask_dump_dir}\"\n",
        "\n",
        "# ---------- Part 2: MiDaS depth + masked point clouds ----------\n",
        "def run_part2(hfov_deg=60.0, min_points=50, depth_scale=1.0):\n",
        "    video_path = SESSION[\"video_path\"]\n",
        "    mask_dump_dir = SESSION[\"mask_dump_dir\"]\n",
        "    out_dir = SESSION[\"out_dir\"]\n",
        "    if not video_path or not mask_dump_dir:\n",
        "        return \"Run Part 1 first.\"\n",
        "\n",
        "    # Load MiDaS\n",
        "    midas = torch.hub.load(\"intel-isl/MiDaS\", \"DPT_Small\").to(device).eval()\n",
        "    midas_transforms = torch.hub.load(\"intel-isl/MiDaS\", \"transforms\")\n",
        "    transform = midas_transforms.small_transform\n",
        "\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    W = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    H = int(cap.get(cv2.CAP_PROP_HEIGHT))\n",
        "    N = int(cap.get(cv2.CAP_PROP_FRAME_COUNT) or 0)\n",
        "\n",
        "    fx = (W/2) / np.tan(np.radians(hfov_deg/2))\n",
        "    fy = fx\n",
        "    cx, cy = W/2, H/2\n",
        "\n",
        "    pc_out_dir = os.path.join(out_dir, \"pointclouds\")\n",
        "    os.makedirs(pc_out_dir, exist_ok=True)\n",
        "\n",
        "    pbar = tqdm(total=N, desc=\"Depth + PLY (CPU)\")\n",
        "    frame_idx = 0\n",
        "    saved = 0\n",
        "    while True:\n",
        "        ok, frame_bgr = cap.read()\n",
        "        if not ok:\n",
        "            break\n",
        "\n",
        "        mask_path = os.path.join(mask_dump_dir, f\"mask_{frame_idx:05d}.png\")\n",
        "        if not os.path.isfile(mask_path):\n",
        "            frame_idx += 1; pbar.update(1); continue\n",
        "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
        "        if mask is None:\n",
        "            frame_idx += 1; pbar.update(1); continue\n",
        "        mask_bool = mask > 127\n",
        "\n",
        "        rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n",
        "        with torch.no_grad():\n",
        "            inp = transform(rgb).to(device)\n",
        "            pred = midas(inp)\n",
        "            pred = torch.nn.functional.interpolate(\n",
        "                pred.unsqueeze(1),\n",
        "                size=(H, W),\n",
        "                mode=\"bicubic\",\n",
        "                align_corners=False\n",
        "            ).squeeze().cpu().numpy().astype(np.float32)\n",
        "\n",
        "        # Normalize to [0,1], avoid zeros, apply optional scale\n",
        "        depth = pred\n",
        "        depth -= depth.min()\n",
        "        denom = depth.max() + 1e-8\n",
        "        depth = (depth / denom + 1e-6) * float(depth_scale)\n",
        "\n",
        "        ys, xs = np.indices((H, W)).astype(np.float32)\n",
        "        X = (xs - cx) / fx * depth\n",
        "        Y = (ys - cy) / fy * depth\n",
        "        Z = depth\n",
        "        XYZ = np.stack([X, Y, Z], axis=-1)\n",
        "\n",
        "        XYZ_obj = XYZ[mask_bool]\n",
        "        RGB_obj = frame_bgr[mask_bool][:, ::-1]  # BGR->RGB\n",
        "\n",
        "        if XYZ_obj.shape[0] >= int(min_points):\n",
        "            ply_path = os.path.join(pc_out_dir, f\"frame_{frame_idx:05d}.ply\")\n",
        "            write_ply_ascii_xyzrgb(ply_path, XYZ_obj.astype(np.float32), RGB_obj.astype(np.uint8))\n",
        "            saved += 1\n",
        "\n",
        "        frame_idx += 1\n",
        "        pbar.update(1)\n",
        "    pbar.close()\n",
        "    cap.release()\n",
        "    return f\"✅ Part 2 done. Saved {saved} PLYs in: {pc_out_dir}\"\n",
        "\n",
        "# ---------------- Build Gradio UI ----------------\n",
        "with gr.Blocks(title=\"SAM2 Single-Object Mask + Point Cloud (CPU)\") as demo:\n",
        "    gr.Markdown(\"## SAM2 (CPU) → Single Object Masked Video → MiDaS Point Clouds\")\n",
        "    gr.Markdown(\"**Step 1. Upload video**\")\n",
        "\n",
        "    with gr.Row():\n",
        "        in_file = gr.File(file_types=[\"video\"], label=\"Video\")\n",
        "        first_frame = gr.Image(label=\"First frame (draw box)\")\n",
        "    msg = gr.Textbox(label=\"Status\", interactive=False)\n",
        "    sess_dump = gr.Code(language=\"json\", label=\"Session (debug)\", interactive=False)\n",
        "\n",
        "    load_btn = gr.Button(\"Load video & preview first frame\")\n",
        "    load_btn.click(fn=load_video, inputs=[in_file], outputs=[first_frame, msg, sess_dump])\n",
        "\n",
        "    gr.Markdown(\"**Step 2. Draw a box around your object, then confirm**\")\n",
        "    bbox_state = gr.State()\n",
        "    set_bbox_btn = gr.Button(\"Use selected box\")\n",
        "    set_bbox_btn.click(fn=set_bbox, inputs=[first_frame], outputs=[msg])\n",
        "\n",
        "    gr.Markdown(\"**Step 3. Run Part 1 (SAM2 tracking)**\")\n",
        "    part1_btn = gr.Button(\"Run Part 1\")\n",
        "    part1_btn.click(fn=run_part1, outputs=[msg])\n",
        "\n",
        "    gr.Markdown(\"**Step 4. Run Part 2 (Depth + Point Clouds)**\")\n",
        "    hfov = gr.Slider(40, 100, value=60, step=1, label=\"Approx HFOV (deg)\")\n",
        "    min_pts = gr.Slider(10, 2000, value=50, step=10, label=\"Min points per PLY\")\n",
        "    dscale = gr.Slider(0.1, 10.0, value=1.0, step=0.1, label=\"Depth scale (MiDaS is relative)\")\n",
        "    part2_btn = gr.Button(\"Run Part 2\")\n",
        "    part2_btn.click(fn=run_part2, inputs=[hfov, min_pts, dscale], outputs=[msg])\n",
        "\n",
        "demo.launch(debug=False, share=True)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f579d24a",
        "outputId": "bf5ee651-ab9c-4174-d236-62d6aeff17fe"
      },
      "source": [
        "# Replace these placeholder values with the actual x, y, width, and height of your bounding box\n",
        "bbox_x = 0  # Example: x-coordinate of the top-left corner\n",
        "bbox_y = 0  # Example: y-coordinate of the top-left corner\n",
        "bbox_w = 100 # Example: width of the bounding box\n",
        "bbox_h = 100 # Example: height of the bounding box\n",
        "\n",
        "SESSION[\"bbox_xywh\"] = (bbox_x, bbox_y, bbox_w, bbox_h)\n",
        "\n",
        "print(f\"Bounding box manually set to: {SESSION['bbox_xywh']}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bounding box manually set to: (0, 0, 100, 100)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "36128cc8",
        "outputId": "13e71644-45bd-4a2b-ad7d-424c7b0d5cff"
      },
      "source": [
        "import json\n",
        "print(\"Current SESSION state:\")\n",
        "print(json.dumps(SESSION, indent=2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current SESSION state:\n",
            "{\n",
            "  \"video_path\": \"/tmp/gradio/27b07c9cdc9ed78a84002bff7dab34ab039a5ff11e725f20c0c9744d64a9b299/ScreenRecording_09-16-2025 06-33-33_1.mov\",\n",
            "  \"first_frame_path\": \"/tmp/tmpryig3pu9/first_frame.jpg\",\n",
            "  \"bbox_xywh\": null,\n",
            "  \"out_dir\": \"/content/ScreenRecording_09-16-2025 06-33-33_1_sam2_single_obj\",\n",
            "  \"masked_video_path\": \"/content/ScreenRecording_09-16-2025 06-33-33_1_sam2_single_obj/masked_object_only.mp4\",\n",
            "  \"mask_dump_dir\": \"/content/ScreenRecording_09-16-2025 06-33-33_1_sam2_single_obj/masks_png\",\n",
            "  \"W\": 526,\n",
            "  \"H\": 512,\n",
            "  \"N\": 429,\n",
            "  \"fps\": 42.17597902670818\n",
            "}\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "authorship_tag": "ABX9TyPcKLjE7rGJ3DvkEiL3dgpk",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}